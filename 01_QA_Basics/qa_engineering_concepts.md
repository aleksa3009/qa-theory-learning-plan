# Core Concepts for QA Engineers

---

## 1. QA Analyst vs QA Engineer

### QA Analyst

A QA Analyst primarily focuses on understanding business requirements and validating that the software behaves according to those requirements.

Typical responsibilities:
- Analyzing user stories and specifications
- Writing detailed test cases
- Executing manual tests
- Reporting and tracking defects
- Performing regression and exploratory testing

The QA Analyst ensures that functionality matches expectations from a business perspective. Strong analytical thinking and attention to detail are essential in this role.

In smaller teams, a QA Analyst may also participate in acceptance testing and communication with stakeholders.

---

### QA Engineer

A QA Engineer includes all responsibilities of a QA Analyst but adds technical depth.

Additional responsibilities:
- Writing and maintaining automated test scripts
- Working with test frameworks (e.g., UI, API, integration automation)
- Integrating tests into CI/CD pipelines
- Reviewing logs and debugging failures
- Collaborating closely with developers on technical issues

A QA Engineer thinks not only about “Does it work?” but also:
- Can we test this efficiently?
- Can we automate this?
- How do we prevent regressions at scale?

For Junior roles, companies may use both titles interchangeably. The important part is understanding that QA Engineer implies stronger technical involvement.

---

## 2. Test Pyramid Principles

The Test Pyramid is a model that describes how automated tests should be distributed across different levels.

### Unit Tests

Unit tests verify individual functions or small components in isolation.

Characteristics:
- Very fast
- Highly stable
- Large in number
- Written mostly by developers

They detect defects early in the development cycle and reduce the cost of fixing bugs.

---

### Integration Tests

Integration tests validate how multiple components interact with each other.

Examples:
- API calls between services
- Database interactions
- Communication between modules

They ensure that contracts and data flows are correct. Integration tests are slower than unit tests but still relatively stable.

---

### End-to-End (E2E) Tests

E2E tests simulate real user behavior across the entire system.

Examples:
- Full login flow
- Checkout process
- User registration with email confirmation

They validate complete business workflows but are:
- Slower
- More fragile
- More expensive to maintain

---

### Core Principle

A healthy project should have:
- Many unit tests
- Some integration tests
- Few E2E tests

This structure provides fast feedback, lower maintenance costs, and higher confidence in releases.

---

## 3. Observability, Logs, and Correlation IDs

### Observability

Observability is the ability to understand what is happening inside a system by analyzing its outputs.

Modern systems often consist of multiple services. When something fails, it is not always obvious where the problem occurred.

Observability relies on:
- Logs
- Metrics
- Traces

QA engineers use these tools to investigate failures and understand system behavior.

---

### Logs

Logs are records of events generated by the application.

They help answer questions such as:
- What request was received?
- What data was processed?
- Why did the system return an error?

Good logging improves debugging efficiency and reduces time to identify root causes.

---

### Correlation IDs

A correlation ID is a unique identifier assigned to a request as it moves through multiple services.

In distributed systems:
- One user action may trigger several backend services.
- Each service logs information separately.

The correlation ID connects all related logs, allowing engineers to trace the full lifecycle of a request.

This is especially important in microservice architectures.

---

## 4. Flaky Tests

### Definition

A flaky test is a test that produces inconsistent results without changes to the application code.

For example:
- The same test passes locally but fails in CI.
- The test fails randomly during high system load.

---

### Common Causes

- Timing issues (race conditions, async behavior)
- Network instability
- Test data dependency
- Environment configuration differences
- Hard-coded waits instead of proper synchronization

---

### Why Flaky Tests Are Dangerous

Flaky tests reduce trust in the test suite.

When teams start ignoring failing tests because they are “usually flaky,” real defects can be missed.

A good QA practice is:
- Never ignore flakiness
- Investigate root cause
- Improve synchronization or test stability
- Use stable test data

---

## 5. SLO, SLA, and Reliability Concepts

### SLO (Service Level Objective)

An SLO is a measurable internal target for system performance or reliability.

Examples:
- 95% of requests respond within 500ms
- System uptime of 99.5%

SLOs define acceptable performance from a technical perspective.

---

### SLA (Service Level Agreement)

An SLA is a formal agreement between a service provider and a client.

It defines guaranteed performance levels and may include penalties if targets are not met.

Example:
- 99.9% uptime guarantee for a production system

---

### Reliability

Reliability means the system consistently performs according to defined expectations.

QA contributes to reliability by:
- Testing under realistic conditions
- Performing regression testing
- Supporting performance and stress testing
- Identifying edge cases and failure scenarios

Understanding reliability helps QA think beyond functional correctness and focus on system stability.

---

## Interview-Ready Summary

- QA Analyst focuses on manual testing and requirements validation.
- QA Engineer adds automation and technical system understanding.
- The Test Pyramid promotes many unit tests, fewer integration tests, and minimal E2E tests.
- Observability helps engineers understand system behavior using logs and tracing.
- Flaky tests must be identified and fixed to maintain test reliability.
- SLOs and SLAs define measurable reliability and performance expectations.